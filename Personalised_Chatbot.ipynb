{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFZMjbPtyXl8",
        "outputId": "bf1b8331-1d54-42d4-d619-fd69990f8800"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sat Feb 17 19:25:36 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQlQ4ZWgcXDW",
        "outputId": "807b249f-afaf-4353-cbca-56c2074802f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.1.0+cu121 requires torch==2.1.0, but you have torch 2.2.0 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 2.2.0 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 2.2.0 which is incompatible.\n",
            "torchvision 0.16.0+cu121 requires torch==2.1.0, but you have torch 2.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -qqq transformers --progress-bar off\n",
        "!pip install -qqq langchain --progress-bar off\n",
        "!pip install -qqq chromadb --progress-bar off\n",
        "!pip install -qqq pypdf --progress-bar off\n",
        "!pip install -qqq xformers --progress-bar off\n",
        "!pip install -qqq sentence_transformers --progress-bar off\n",
        "!pip install -qqq InstructorEmbedding --progress-bar off\n",
        "!pip install -qqq pdf2image --progress-bar off"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63OzQKVZKOFq",
        "outputId": "4e27c7a6-ce9c-49c7-ffd2-c8dd85885d69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sentence-transformers==2.2.2\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/86.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (4.35.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (4.66.2)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (2.2.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (0.16.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (1.11.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (4.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6.0->sentence-transformers==2.2.2) (12.3.101)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.4.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers==2.2.2) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers==2.2.2) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers==2.2.2) (3.2.0)\n",
            "Collecting torch>=1.6.0 (from sentence-transformers==2.2.2)\n",
            "  Downloading torch-2.1.0-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers==2.2.2) (9.4.0)\n",
            "Collecting nvidia-nccl-cu12==2.18.1 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.1.0 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers==2.2.2) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers==2.2.2) (1.3.0)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=34d0ad753411fca885e5593eefe1256349bdf5f891f91b399897a7907d43a583\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: triton, nvidia-nccl-cu12, torch, sentence-transformers\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.2.0\n",
            "    Uninstalling triton-2.2.0:\n",
            "      Successfully uninstalled triton-2.2.0\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.19.3\n",
            "    Uninstalling nvidia-nccl-cu12-2.19.3:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.19.3\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.2.0\n",
            "    Uninstalling torch-2.2.0:\n",
            "      Successfully uninstalled torch-2.2.0\n",
            "  Attempting uninstall: sentence-transformers\n",
            "    Found existing installation: sentence-transformers 2.3.1\n",
            "    Uninstalling sentence-transformers-2.3.1:\n",
            "      Successfully uninstalled sentence-transformers-2.3.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xformers 0.0.24 requires torch==2.2.0, but you have torch 2.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-nccl-cu12-2.18.1 sentence-transformers-2.2.2 torch-2.1.0 triton-2.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence-transformers==2.2.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecTRR8hwhn0C"
      },
      "outputs": [],
      "source": [
        "!wget -q https://github.com/PanQiWei/AutoGPTQ/releases/download/v0.4.1/auto_gptq-0.4.1+cu118-cp310-cp310-linux_x86_64.whl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0_Fa50LkfoB",
        "outputId": "a7c94dcc-c3ad-45c3-fb43-4f065d66fd4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ibis-framework 7.1.0 requires pyarrow<15,>=2, but you have pyarrow 15.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -qqq auto_gptq-0.4.1+cu118-cp310-cp310-linux_x86_64.whl --progress-bar off"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7ShwfeTubDU",
        "outputId": "eaa34f36-d137-4c1f-e404-0212c73133ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 33 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 696 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.3 [186 kB]\n",
            "Fetched 186 kB in 1s (214 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 121749 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.3_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.3) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.3) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get install poppler-utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vH5IpjeNflMU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from auto_gptq import AutoGPTQForCausalLM\n",
        "from langchain import HuggingFacePipeline, PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from pdf2image import convert_from_path\n",
        "from transformers import AutoTokenizer, TextStreamer, pipeline\n",
        "\n",
        "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiX57tzWOJAg"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQm1m0Me0XHj"
      },
      "outputs": [],
      "source": [
        "!mkdir pdfs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8yAnlojofKq_",
        "outputId": "d8c27ebe-ba3a-4a2e-ebdd-f183681ff518"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4NSGmNp7nEwU"
      },
      "outputs": [],
      "source": [
        "from pdf2image import convert_from_path\n",
        "import os\n",
        "\n",
        "# Path to the folder containing the PDFs\n",
        "folder_path = \"/content/gdrive/MyDrive/AI   ML Research Engg Data/SSD-20724880\"\n",
        "\n",
        "# List to store the image paths\n",
        "image_paths = []\n",
        "\n",
        "# Iterate over the files in the folder\n",
        "for filename in os.listdir(folder_path):\n",
        "    if filename.endswith(\".pdf\"):\n",
        "        pdf_path = os.path.join(folder_path, filename)\n",
        "        images = convert_from_path(pdf_path, dpi=88)\n",
        "\n",
        "        image_paths.append(images)\n",
        "\n",
        "# The image_paths list now contains the paths to the generated images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dZtUHaJOqH5o"
      },
      "outputs": [],
      "source": [
        "!gdown 1v-Rn1FVU1pLTAQEgm0N9oB6cExMoebZr -O pdfs/tesla-earnings-report.pdf\n",
        "!gdown 1Xc890jrQvCExAkryVWAttsv1DBLdVefN -O pdfs/nvidia-earnings-report.pdf\n",
        "!gdown 1Epz-SQ3idPpoz75GlTzzomag8gplzLv8 -O pdfs/meta-earnings-report.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "a9X4Fbv8ugOW"
      },
      "outputs": [],
      "source": [
        "meta_images = convert_from_path(\"pdfs/meta-earnings-report.pdf\", dpi=88)\n",
        "meta_images[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "We2A7lVtukAb"
      },
      "outputs": [],
      "source": [
        "nvidia_images = convert_from_path(\"pdfs/nvidia-earnings-report.pdf\", dpi=88)\n",
        "nvidia_images[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yL1bVMC2twkB"
      },
      "outputs": [],
      "source": [
        "tesla_images = convert_from_path(\"pdfs/tesla-earnings-report.pdf\", dpi=88)\n",
        "tesla_images[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6ywUvMDqZFf4"
      },
      "outputs": [],
      "source": [
        "!rm -rf \"db\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9G0U1_hUr7C-"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "!pip install kor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RQmhREpeOInh"
      },
      "outputs": [],
      "source": [
        "loader = PyPDFDirectoryLoader(\"/content/pdfs\")\n",
        "docs = loader.load()\n",
        "len(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wjJ0DI9MRNkT"
      },
      "outputs": [],
      "source": [
        "embeddings = HuggingFaceInstructEmbeddings(\n",
        "    model_name=\"hkunlp/instructor-large\", model_kwargs={\"device\": DEVICE}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yFztsob3WO3x"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=64)\n",
        "texts = text_splitter.split_documents(docs)\n",
        "len(texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ESFJOOYEWTX9"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "db = Chroma.from_documents(texts, embeddings, persist_directory=\"db1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcylvlF60r_m"
      },
      "source": [
        "## Llama 2 7B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DrSTLMQbfjFp"
      },
      "outputs": [],
      "source": [
        "model_name_or_path = \"TheBloke/Llama-2-7B-chat-GPTQ\"\n",
        "model_basename = \"model\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
        "\n",
        "model = AutoGPTQForCausalLM.from_quantized(\n",
        "    model_name_or_path,\n",
        "    revision=\"gptq-4bit-128g-actorder_True\",\n",
        "    model_basename=model_basename,\n",
        "    use_safetensors=True,\n",
        "    trust_remote_code=True,\n",
        "    inject_fused_attention=False,\n",
        "    device=DEVICE,\n",
        "    quantize_config=None,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "k6yLtagJfvG4"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NKUo_0gxRD-a"
      },
      "outputs": [],
      "source": [
        "DEFAULT_SYSTEM_PROMPT = \"\"\"\n",
        "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
        "\n",
        "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "def generate_prompt(prompt: str, system_prompt: str = DEFAULT_SYSTEM_PROMPT) -> str:\n",
        "    return f\"\"\"\n",
        "[INST] <<SYS>>\n",
        "{system_prompt}\n",
        "<</SYS>>\n",
        "\n",
        "{prompt} [/INST]\n",
        "\"\"\".strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5H6rPBpEAALY"
      },
      "outputs": [],
      "source": [
        "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZYaQDkDvVvIz"
      },
      "outputs": [],
      "source": [
        "text_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=256,\n",
        "    temperature=0,\n",
        "    top_p=0.95,\n",
        "    repetition_penalty=1.15,\n",
        "    streamer=streamer,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "s0sKzP77ZCok"
      },
      "outputs": [],
      "source": [
        "llm = HuggingFacePipeline(pipeline=text_pipeline, model_kwargs={\"temperature\": 0})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnjLdHDAhTT-"
      },
      "source": [
        "## \"Act as\" Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "J3RKlMe4DNMn"
      },
      "outputs": [],
      "source": [
        "SYSTEM_PROMPT = \"I need you to assume your identity as Hirthik (i.e 'I want you to act as me'). Respond to only questions that is related to me (Hirthik) from a first-person perspective, as if you were me. All the details provided pertain to my personal information. Utilize only the provided context to address the question provided. First check if the question is related to me and then answer the question.\"\n",
        "\n",
        "template = generate_prompt(\n",
        "    \"\"\"\n",
        "Use only the following piece of context to answer the question at the end. Do not use any outside knowledge. If you don't know the answer say you don't know.\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "\"\"\",\n",
        "    system_prompt=SYSTEM_PROMPT,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pJpTuSEoDDEJ"
      },
      "outputs": [],
      "source": [
        "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YlzcQwjwWHGB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0DC-RU6DZCIS"
      },
      "outputs": [],
      "source": [
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=db.as_retriever(search_kwargs={\"k\": 2}),\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs={\"prompt\": prompt},\n",
        "    # verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7IWZ6H1ax4z"
      },
      "source": [
        "## Chat with Multiple PDFs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_BR5nBf_fPao"
      },
      "outputs": [],
      "source": [
        "result = qa_chain(\"Which year are you graduating?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UyyiaN0yZ6LN"
      },
      "outputs": [],
      "source": [
        "len(result[\"source_documents\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "60qpCy_haA3D"
      },
      "outputs": [],
      "source": [
        "print(result[\"source_documents\"][0].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nKmLM4UTfVmL"
      },
      "outputs": [],
      "source": [
        "result = qa_chain(\"Do you have experience as a Data Scientist?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1k96dxrgfjMr"
      },
      "outputs": [],
      "source": [
        "result = qa_chain(\"What is the per share revenue for Nvidia during 2023?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kpMIspiSahlD"
      },
      "outputs": [],
      "source": [
        "print(result[\"source_documents\"][1].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "waIck-yefydp"
      },
      "outputs": [],
      "source": [
        "result = qa_chain(\"What is the capital of India?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nq6cf8vof6wM"
      },
      "outputs": [],
      "source": [
        "result = qa_chain(\"What is the estimated YOY revenue for Tesla during 2023?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SIFTDoxzgY07"
      },
      "outputs": [],
      "source": [
        "result = qa_chain(\"What is the estimated YOY revenue for Nvidia during 2023?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cxp167huggFb"
      },
      "outputs": [],
      "source": [
        "result = qa_chain(\n",
        "    \"Which company is more profitable during 2023 Meta, Nvidia or Tesla and why?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vRuJYwSbgpij"
      },
      "outputs": [],
      "source": [
        "result = qa_chain(\n",
        "    \"Choose one company to invest (Tesla, Nvidia or Meta) to maximize your profits for the long term (10+ years)?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdyKOEdRg2hc"
      },
      "source": [
        "## References\n",
        "\n",
        "- [Tesla Quarterly Report (Jul 21, 2023)](https://ir.tesla.com/_flysystem/s3/sec/000095017023033872/tsla-20230630-gen.pdf)\n",
        "- [Meta Q2 2023 Earnings (Jul 26, 2023)](https://s21.q4cdn.com/399680738/files/doc_financials/2023/q2/Meta-06-30-2023-Exhibit-99-1-FINAL.pdf)\n",
        "- [Nvidia Fiscal Q1 2024](https://s201.q4cdn.com/141608511/files/doc_financials/2024/q1/ecefb2b2-efcb-45f3-b72b-212d90fcd873.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zbtUhnMUSehk"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}